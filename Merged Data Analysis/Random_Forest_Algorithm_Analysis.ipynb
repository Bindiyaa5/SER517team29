{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0\n",
      "R² Score: 1.0\n",
      "                       Coefficient\n",
      "start_lat                      0.0\n",
      "start_lng                      0.0\n",
      "end_lat                        0.0\n",
      "end_lng                        0.0\n",
      "Start_Altitude                 0.0\n",
      "End_Altitude                   0.0\n",
      "end_altitude                   0.0\n",
      "end_Altitude                   0.0\n",
      "Start_Altutude                 0.0\n",
      "State_Altitude                 0.0\n",
      "Elevation_Change               0.0\n",
      "member_casual_Unknown          0.0\n",
      "member_casual_casual           0.0\n",
      "member_casual_member           0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtokala\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\jtokala\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.0\n",
      "F1-Score: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   3416267\n",
      "\n",
      "    accuracy                           1.00   3416267\n",
      "   macro avg       1.00      1.00      1.00   3416267\n",
      "weighted avg       1.00      1.00      1.00   3416267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df = pd.read_csv('merged_rides__with_elevation.csv')\n",
    "\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)  # Only fill numeric columns with their means\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n",
    "target_variable = 'rideable_type'\n",
    "y = df[target_variable]\n",
    "X = df.drop(target_variable, axis=1)\n",
    "\n",
    "# Apply encoding to the features\n",
    "label_encoders = {}\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':  # This identifies categorical columns\n",
    "        num_unique_values = len(X[column].unique())\n",
    "        if num_unique_values < 10:  # Limit set for one-hot encoding\n",
    "            # Apply one-hot encoding to columns with fewer unique values\n",
    "            dummies = pd.get_dummies(X[column], prefix=column)\n",
    "            X = pd.concat([X, dummies], axis=1)\n",
    "        else:\n",
    "            # Apply label encoding to columns with many unique values to save memory\n",
    "            label_encoders[column] = LabelEncoder()\n",
    "            X[column] = label_encoders[column].fit_transform(X[column].astype(str))\n",
    "        X.drop(column, axis=1, inplace=True)  # Drop original column after encoding\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train_encoded = (y_train == 'electric').astype(int)  # Encode 'electric' as 1, 'classic' as 0\n",
    "y_test_encoded = (y_test == 'electric').astype(int)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming X_train, X_test, y_train, and y_test have already been defined and split appropriately\n",
    "\n",
    "# Encode 'electric' as 1, 'classic' as 0 in both training and testing labels\n",
    "y_train_encoded = (y_train == 'electric').astype(int)\n",
    "y_test_encoded = (y_test == 'electric').astype(int)\n",
    "\n",
    "# Initialize the LinearRegression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data with encoded labels\n",
    "model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating R-squared and Mean Squared Error based on predictions and the true test labels\n",
    "mse = mean_squared_error(y_test_encoded, y_pred)\n",
    "r2 = r2_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Displaying coefficients\n",
    "coefficients = pd.DataFrame(model.coef_, X_train.columns, columns=['Coefficient'])\n",
    "print(coefficients)\n",
    "\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, classification_report\n",
    "\n",
    "# Predict on the test set (assuming `model` has already been trained on `X_train, y_train_encoded`)\n",
    "y_pred = model.predict(X_test)  # This should be predictions on the test set\n",
    "y_pred_encoded = (y_pred > 0.5).astype(int)  # Assuming you're using a threshold of 0.5 if needed\n",
    "\n",
    "# Calculate recall, F1 score using the encoded test labels and predictions\n",
    "recall = recall_score(y_test_encoded, y_pred_encoded)  # pos_label defaults to 1 which is suitable here\n",
    "f1 = f1_score(y_test_encoded, y_pred_encoded)\n",
    "\n",
    "# Print recall and F1-score\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "\n",
    "# For a comprehensive report including precision and support per class\n",
    "report = classification_report(y_test_encoded, y_pred_encoded)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Feature  Importance\n",
      "1               start_lng    0.322430\n",
      "3                 end_lng    0.226781\n",
      "0               start_lat    0.166575\n",
      "2                 end_lat    0.091401\n",
      "4          Start_Altitude    0.082592\n",
      "5            End_Altitude    0.034461\n",
      "12   member_casual_casual    0.030882\n",
      "13   member_casual_member    0.024493\n",
      "10       Elevation_Change    0.013673\n",
      "7            end_Altitude    0.005454\n",
      "9          State_Altitude    0.000680\n",
      "8          Start_Altutude    0.000519\n",
      "6            end_altitude    0.000058\n",
      "11  member_casual_Unknown    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming RandomForestClassifier is fitted as `rf`\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)  # Limiting depth\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importancesz\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to view the features and their importance scores\n",
    "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision by class: [0.63066374 0.69230769 0.7236172 ]\n",
      "Recall by class: [7.51173112e-01 1.17504211e-04 6.29748790e-01]\n",
      "F1-Score by class: [6.85663641e-01 2.34968540e-04 6.73427673e-01]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " classic_bike       0.63      0.75      0.69   1589149\n",
      "  docked_bike       0.69      0.00      0.00     76593\n",
      "electric_bike       0.72      0.63      0.67   1750525\n",
      "\n",
      "     accuracy                           0.67   3416267\n",
      "    macro avg       0.68      0.46      0.45   3416267\n",
      " weighted avg       0.68      0.67      0.66   3416267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming `rf` is your trained RandomForestClassifier\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "# Calculate metrics for each class\n",
    "precision = precision_score(y_test, y_pred, average=None)  # returns an array for each class\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "print(\"Precision by class:\", precision)\n",
    "print(\"Recall by class:\", recall)\n",
    "print(\"F1-Score by class:\", f1)\n",
    "# Print a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
