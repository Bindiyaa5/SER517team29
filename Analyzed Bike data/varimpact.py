# -*- coding: utf-8 -*-
"""varImpact.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10k5UObkL9O_mFp0UJK6utZZCRH7Vuo9T
"""

from google.colab import drive
drive.mount('/content/drive')

filepath = '/content/drive/My Drive/merged_rides_with_elevation.csv'

import pandas as pd
df = pd.read_csv(filepath)

df['started_at'] = pd.to_datetime(df['started_at'])
df['ended_at'] = pd.to_datetime(df['ended_at'])

df['trip_duration'] = df['ended_at'] - df['started_at']

from sklearn.impute import SimpleImputer
df['Elevation_Change'] = df['End_Altitude'] - df['Start_Altitude']
# Impute the missing values in 'Elevation_Change'
imputer = SimpleImputer(strategy='mean')  # or strategy='median' if that's more appropriate
df['Elevation_Change'] = imputer.fit_transform(df[['Elevation_Change']])

# Continue with your model fitting as before

df = df.drop(columns=['end_altitude', 'end_Altitude', 'Start_Altutude', 'State_Altitude'])

# Replace missing values with some form of imputation if necessary
# For now, we'll just drop rows with NaN in 'End_Altitude'
df = df.dropna(subset=['End_Altitude'])

df['started_at'] = pd.to_datetime(df['started_at'])
df['ended_at'] = pd.to_datetime(df['ended_at'])

import numpy as np

# Function to calculate distance between two lat-lng points
def haversine_distance(lat1, lon1, lat2, lon2):
    # Radius of the Earth in kilometers
    R = 6371.0
    # Convert latitude and longitude from degrees to radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    # Difference in coordinates
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    # Haversine formula
    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    # Distance in kilometers
    distance = R * c
    return distance

# Apply the function to your DataFrame

df['Distance'] = df.apply(lambda x: haversine_distance(x['start_lat'], x['start_lng'], x['end_lat'], x['end_lng']), axis=1)

df = df.drop(columns=['start_lat', 'start_lng', 'end_lat', 'end_lng'])

df['rideable_type'] = df['rideable_type'].replace({
    'electric_bike': 'electric',
    'docked_bike': 'classic',
    'classic_bike': 'classic'
})

columns_to_drop = ['Start_Altitude', 'End_Altitude', 'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id']
df = df.drop(columns=columns_to_drop)

X = df.drop(columns=['rideable_type', 'Elevation_Change'])
column_types = X.dtypes
print(column_types)

X

y = df['rideable_type']
column_types = y.dtypes
print(column_types)

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the data
# Assuming df_X is the DataFrame X and series_y is the Series y
# Make sure the indices are aligned correctly

# EDA
# Check the first few rows of X and y
print(X.head())
print(y.head())

# Summary statistics of X
print(X.describe())

# Distribution of y
sns.countplot(x=y)
plt.show()

# Check the relationship between numerical variables in X and y
sns.pairplot(pd.concat([X, y], axis=1), hue='rideable_type')
plt.show()

# Check the relationship between categorical variables in X and y
# For example, 'member_casual' and 'rideable_type'
sns.countplot(x='member_casual', hue='rideable_type', data=pd.concat([X, y], axis=1))
plt.show()

# Feature Engineering (if needed)
# For example, converting categorical variables to numerical using LabelEncoder
le = LabelEncoder()
X['member_casual_encoded'] = le.fit_transform(X['member_casual'])

# Modeling
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the data types of all columns in X_train
print(X_train.dtypes)

# Convert timedelta column to seconds
X_train['trip_duration_seconds'] = X_train['trip_duration'].dt.total_seconds()

# Remove original timedelta column
X_train.drop(columns=['trip_duration'], inplace=True)

# Convert X_train to a numpy array
X_train_array = X_train.to_numpy()

# Train a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)


rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test_encoded, y_pred)
print("Accuracy:", accuracy)

print(classification_report(y_test_encoded, y_pred))

# Convert trip_duration_seconds column to integer
X_train['trip_duration_seconds'] = X_train['trip_duration_seconds'].astype(int)

# Check the data types of all columns in X_train
print(X_train.dtypes)

# Check the data type of y_train
print(y_train.dtype)

# Train a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit label encoder on categorical columns in X_train
for column in X_train.columns:
    # Check if the column is of object type (i.e., categorical)
    if X_train[column].dtype == 'object':
        # Fit label encoder on the column
        label_encoder.fit(X_train[column])

# Transform categorical columns in both X_train and X_test
for column in X_train.columns:
    if X_train[column].dtype == 'object':
        X_train[column] = label_encoder.transform(X_train[column])
        X_test[column] = label_encoder.transform(X_test[column])

# Get data types of X_train
print(X_train.dtypes)
# Get data type of y_train_encoded
print(y_train_encoded.dtype)

print(X_train.dtypes)

print(X_train.isnull().sum())

X_train['Distance'].fillna(X_train['Distance'].mean(), inplace=True)

# Remove rows with missing values
X_train.dropna(inplace=True)

print("Data types of X_train:\n", X_train.dtypes)
print("\nData type of y_train_series:", y_train_series.dtype)

print("Data type of X_train_array:", X_train_array.dtype)
print("Data type of y_train_series:", y_train_series.dtype)

# Convert timedelta column to seconds
X_train['trip_duration_seconds'] = X_train['trip_duration'].dt.total_seconds()

# Remove original timedelta column
X_train.drop(columns=['trip_duration'], inplace=True)

# Convert X_train to a numpy array
X_train_array = X_train.to_numpy()

# Train a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)


rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test_encoded, y_pred)
print("Accuracy:", accuracy)

print(classification_report(y_test_encoded, y_pred))

# Convert y_train_encoded back to pandas Series with data type int64
y_train_series = pd.Series(y_train_encoded, dtype='int64')

# Train a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

rf_classifier.fit(X_train, y_train_series)





y

import statsmodels.api as sm
X = sm.add_constant(X)

X.fillna(X.mean(), inplace=True)

model = sm.OLS(y, X).fit()


# Step 4: Analyze Coefficients
# Examine the coefficients of the regression model
print(model.summary())

print("R-squared:", model.rsquared)
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib.pyplot as plt
import seaborn as sns
# Step 5: Check for multicollinearity
# Calculate VIF for each independent variable
vif = pd.DataFrame()
vif["Variable"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif)

# Step 6: Check model assumptions
# Residual plot
plt.figure(figsize=(8, 6))
sns.residplot(x=model.fittedvalues, y=model.resid)
plt.title("Residual Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.show()

# QQ plot of residuals
sm.qqplot(model.resid, line='s')
plt.title("Q-Q Plot of Residuals")
plt.show()

rf_classifier = RandomForestClassifier(
    n_estimators=50,      # Reduced number of trees
    max_depth=15,         # Limiting tree depth
    max_features='sqrt',  # Limiting the number of features considered for splits
    min_samples_split=10, # Minimum number of samples required to split an internal node
    min_samples_leaf=5,   # Minimum number of samples required to be at a leaf node
    random_state=42,      # For reproducibility
    class_weight='balanced', # Handling imbalanced classes
    n_jobs=-1             # Use all cores available
)

from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest model
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_classifier.fit(X_train, y_train)

# Predict on the testing set
y_pred_rf = rf_classifier.predict(X_test)

# Evaluate the model
print("Random Forest - Classification Report:")
print(classification_report(y_test, y_pred_rf))

import pandas as pd
import numpy as np
from statsmodels.discrete.discrete_model import Poisson
from sklearn.preprocessing import LabelEncoder

# Assuming df is your DataFrame containing X and y
# Ensure proper data types

# # Convert datetime columns to numeric representation if needed
# df['started_at'] = (df['started_at'] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1s')
# df['ended_at'] = (df['ended_at'] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1s')

# Encode categorical variables if needed
label_encoder = LabelEncoder()
df['member_casual'] = label_encoder.fit_transform(df['member_casual'])

# Define y and X
y = df['rideable_type'].astype('category').cat.codes  # Convert categorical to numeric codes
X = df[['started_at', 'ended_at', 'member_casual', 'Elevation_Change', 'Distance']]

import numpy as np
from statsmodels.discrete.discrete_model import Poisson

# Check for missing or infinite values in X
print("Missing or Infinite Values in X:")
print(np.isnan(X).sum())
print(np.isinf(X).sum())

# Handle missing or infinite values in X
# For example, you can impute missing values with the mean
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Re-fit the Poisson model
poisson_model = Poisson(y, X_imputed).fit()

# Analyze results
print(poisson_model.summary())

y = np.asarray(y)
X = np.asarray(X)

# Then, re-run the model fitting
# poisson_model = Poisson(y, X).fit()

from statsmodels.discrete.discrete_model import Poisson
poisson_model = Poisson(y, X_clean).fit()

import numpy as np
from statsmodels.discrete.discrete_model import Poisson
# Handle missing or infinite values in X
X_clean = np.nan_to_num(X, nan=np.nanmean(X), posinf=np.nanmean(X), neginf=np.nanmean(X))

# Then, re-run the model fitting
poisson_model = Poisson(y, X_clean).fit()

poisson_predictions = poisson_model.predict(X)

poisson_predictions

